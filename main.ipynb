{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e57dea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force IPython to auto-flush outputs\n",
    "%config Application.log_level = 'INFO' \n",
    "%config InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20df530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import sys\n",
    "import os\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea354f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53088d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crimson leaves descend,  \n",
      "Whispers dance on crisp cool breeze,  \n",
      "Autumn’s soft farewell.\n",
      "content='Crimson leaves descend,  \\nWhispers dance on crisp cool breeze,  \\nAutumn’s soft farewell.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 26, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CItH3BIst8zKQFqwJ0J1lX1t3CcGN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--de10177a-5788-4e26-8eb3-d2087025252b-0' usage_metadata={'input_tokens': 26, 'output_tokens': 21, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Single LLM Call\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  # best practice\n",
    "    request_timeout=120,\n",
    "    max_retries=5,\n",
    "    verbose=True,\n",
    "   \n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"assistant\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"Write me a haiku about autumn leaves.\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "res = llm.invoke(messages)   # list-of-conversations -> ChatResult\n",
    "\n",
    "print(res.content) \n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcfadaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"J'aime la programmation.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "844b5ae3",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CItZVev8iuk3B0azTCntjYREfl0DF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--455f47d8-6d62-4b2b-a5f9-08000532f5ca-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Crimson leaves descend,  \\nWhispers of the cooling breeze,  \\nAutumn’s soft farewell.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 16, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CItZVKwZPA0vgTFgmdOm3Ozmpl58y', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7a4ef74-63ef-44d0-aedb-c4fdcbf85601-0', usage_metadata={'input_tokens': 16, 'output_tokens': 20, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4fce0778af', 'id': 'chatcmpl-CItZV8JkbrZoWTJkradfqvX9Emgtp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e1ef8f18-46a6-4ac3-bc66-6598f0aa1045-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Crimson leaves descend,  \\nWhispers of the cooling breeze,  \\nAutumn’s soft farewell.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 16, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CItZVPJD8rHWdUZBsTSUokaSfJyUU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5afc4761-f9d8-495e-a256-7a594be2f6b5-0', usage_metadata={'input_tokens': 16, 'output_tokens': 20, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 18, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CItZVXcS13nXpVSIRNT3x9BuxKu8o', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--332de955-9330-4c7a-9bfe-c7535ae7fb6e-0', usage_metadata={'input_tokens': 18, 'output_tokens': 9, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Crimson leaves descend,  \\nWhispers of the cooling breeze,  \\nAutumn’s soft farewell.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 16, 'total_tokens': 36, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_6d7dcc9a98', 'id': 'chatcmpl-CItZVc0NVji4h5qJanIAWpjEqTpCk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9946a2b4-11f2-4476-ada2-f7ea069b0f56-0', usage_metadata={'input_tokens': 16, 'output_tokens': 20, 'total_tokens': 36, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "\n",
      "\n",
      "CHOICE 1\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "CHOICE 2\n",
      "\n",
      "Crimson leaves descend,  \n",
      "Whispers of the cooling breeze,  \n",
      "Autumn’s soft farewell.\n",
      "\n",
      "\n",
      "CHOICE 3\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "CHOICE 4\n",
      "\n",
      "Crimson leaves descend,  \n",
      "Whispers of the cooling breeze,  \n",
      "Autumn’s soft farewell.\n",
      "\n",
      "\n",
      "CHOICE 5\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "\n",
      "CHOICE 6\n",
      "\n",
      "Crimson leaves descend,  \n",
      "Whispers of the cooling breeze,  \n",
      "Autumn’s soft farewell.\n"
     ]
    }
   ],
   "source": [
    "if 1: #this is just a filter to allow or block cell execution\n",
    "    # Batched LLM Calls\n",
    "    from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "    messages = [\n",
    "        (\"assistant\", \"You are a helpful assistant.\"),\n",
    "        ([HumanMessage(\"Write me a haiku about autumn leaves.\")]),\n",
    "    ] * 3\n",
    "\n",
    "    completion = llm.batch(messages)  # list-of-list-of-conversations -> ChatResult\n",
    "\n",
    "    print(completion)\n",
    "\n",
    "    for i, choice in enumerate(completion):\n",
    "        print(f\"\\n\\nCHOICE {i + 1}\\n\")\n",
    "        print(choice.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using template allows use insert parameters into the prompt - you only need to provide parameter values at query time\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"French\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.0)\n",
    "\n",
    "# --- First prompt: reformulate ---\n",
    "reformulate_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that rewrites questions in a clearer, more formal way.\"),\n",
    "    (\"user\", \"Rewrite the following question to be clear and formal: {raw_question}\")\n",
    "])\n",
    "\n",
    "# --- Second prompt: answer the reformulated question ---\n",
    "answer_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a knowledgeable tutor.\"),\n",
    "    (\"user\", \"Answer this question in a concise and friendly way:\\n\\n{clean_question}\")\n",
    "])\n",
    "\n",
    "# --- Chain them together ---\n",
    "# reformulate → llm → inject into second prompt → llm\n",
    "chain = (\n",
    "    reformulate_prompt \n",
    "    | llm \n",
    "    | (lambda msg: {\"clean_question\": msg.content})  # extract content\n",
    "    | answer_prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "# --- Run the chain ---\n",
    "query = \"hey what is chatprompttemplate used for???\"\n",
    "response = chain.invoke({\"raw_question\": query})\n",
    "\n",
    "print(\"\\n Input prompt:\", query)\n",
    "print(response.content)\n",
    "\n",
    "# Run only the first prompt + LLM\n",
    "first_result = (reformulate_prompt | llm).invoke({\"raw_question\": query})\n",
    "print(\"\\n Before reformulation:\", query)\n",
    "print(\"\\n After reformulation:\", first_result.content)\n",
    "\n",
    "second_result = (answer_prompt | llm).invoke({\"clean_question\": first_result.content})\n",
    "print(\"\\n After answering:\", second_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51169d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    # how to have an interactive real time conversation with the assistant?\n",
    "\n",
    "    # Initialise model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "    def ask_once(user_input: str):\n",
    "        # Only system + user, no history\n",
    "        messages = [\n",
    "            SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "            HumanMessage(content=user_input)\n",
    "        ]\n",
    "        response = llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    # Run a simple q&a session\n",
    "    i=0\n",
    "    while True:\n",
    "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
    "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
    "            break\n",
    "        answer = ask_once(user_input)\n",
    "        i += 1\n",
    "        print(f\"Assistant: {answer}\\n\")\n",
    "\n",
    "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ecfb1",
   "metadata": {},
   "source": [
    "What did you notice about this Assistant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ae6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    # What is diffeernt about this version?\n",
    "\n",
    "    # initialise model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "    # start with a system prompt\n",
    "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "\n",
    "    # function to ask user input and continue the conversation\n",
    "    def chat_turn(user_input: str):\n",
    "        # append user message\n",
    "        history.append(HumanMessage(content=user_input))\n",
    "\n",
    "        # call model with full history\n",
    "        ai_msg = llm.invoke(history)  # returns an AIMessage\n",
    "        #print(f\"Assistant: {ai_msg.content}\")\n",
    "\n",
    "        # append assistant reply manually (so it's fed into the next turn)\n",
    "        history.append(ai_msg)\n",
    "        return ai_msg.content\n",
    "\n",
    "    # Run a simple q&a session\n",
    "    i=0\n",
    "    while True:\n",
    "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
    "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
    "            break\n",
    "        answer = chat_turn(user_input)\n",
    "        i += 1\n",
    "        print(f\"Assistant: {answer}\\n\")\n",
    "\n",
    "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5f4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: As of September 2025, the Prime Minister of the United Kingdom is Sir Keir Starmer. He assumed office on 5 July 2024, following the Labour Party's victory in the general election. ([gov.uk](https://www.gov.uk/government/people/keir-starmer?utm_source=openai)) Prior to his premiership, Starmer served as the Leader of the Opposition from April 2020 to July 2024. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Keir_Starmer?utm_source=openai))\n",
      "\n",
      "In September 2025, Starmer appointed David Lammy as Deputy Prime Minister and Secretary of State for Justice. ([en.wikipedia.org](https://en.wikipedia.org/wiki/David_Lammy?utm_source=openai))\n",
      "\n",
      "\n",
      "## Recent Developments in UK Politics:\n",
      "- [UK's Starmer to press ahead with digital ID plans, FT reports](https://www.reuters.com/world/uk/uks-starmer-set-push-digital-id-plans-ft-reports-2025-09-19/?utm_source=openai)\n",
      "- [Trump begins historic state visit to UK amid pomp and protests](https://www.reuters.com/world/uk/trump-begins-historic-state-visit-uk-amid-pomp-protests-2025-09-17/?utm_source=openai)\n",
      "- [U.S. state visit yields record 150 billion pounds of investment, UK says](https://www.reuters.com/world/uk/us-state-visit-yields-record-150-billion-pounds-investment-uk-says-2025-09-17/?utm_source=openai) \n",
      "\n",
      "{\"As of September 2025, the Prime Minister of the United Kingdom is Sir Keir Starmer. He assumed office on 5 July 2024, following the Labour Party's victory in the general election. ([gov.uk](https://www.gov.uk/government/people/keir-starmer?utm_source=openai)) Prior to his premiership, Starmer served as the Leader of the Opposition from April 2020 to July 2024. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Keir_Starmer?utm_source=openai))\\n\\nIn September 2025, Starmer appointed David Lammy as Deputy Prime Minister and Secretary of State for Justice. ([en.wikipedia.org](https://en.wikipedia.org/wiki/David_Lammy?utm_source=openai))\\n\\n\\n## Recent Developments in UK Politics:\\n- [UK's Starmer to press ahead with digital ID plans, FT reports](https://www.reuters.com/world/uk/uks-starmer-set-push-digital-id-plans-ft-reports-2025-09-19/?utm_source=openai)\\n- [Trump begins historic state visit to UK amid pomp and protests](https://www.reuters.com/world/uk/trump-begins-historic-state-visit-uk-amid-pomp-protests-2025-09-17/?utm_source=openai)\\n- [U.S. state visit yields record 150 billion pounds of investment, UK says](https://www.reuters.com/world/uk/us-state-visit-yields-record-150-billion-pounds-investment-uk-says-2025-09-17/?utm_source=openai) \"}\n"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    # And about this version?\n",
    "\n",
    "    # initialise model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0,output_version=\"responses/v1\")\n",
    "\n",
    "    tool = {\"type\": \"web_search_preview\"}\n",
    "    llm_with_tools = llm.bind_tools([tool])\n",
    "\n",
    "    # start with a system prompt\n",
    "    history = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "\n",
    "    # function to ask user input and continue the conversation\n",
    "    def chat_turn(user_input: str):\n",
    "        # append user message\n",
    "        history.append(HumanMessage(content=user_input))\n",
    "\n",
    "        # call model with full history\n",
    "        ai_msg = llm_with_tools.invoke(history)  # returns an AIMessage\n",
    "        #print(f\"Assistant: {ai_msg.content}\")\n",
    "\n",
    "        # append assistant reply manually (so it's fed into the next turn)\n",
    "        history.append(ai_msg)\n",
    "        return ai_msg.text()\n",
    "\n",
    "    # Run a simple q&a session\n",
    "    i=0\n",
    "    while True:\n",
    "        user_input = input(f\"Q{i+1}: \")   # prompt user in Jupyter cell\n",
    "        if user_input == \"quit\" or user_input == \"exit\" or user_input == \"break\" or not user_input.strip():        # allow empty string or quit or exit or break to break early\n",
    "            break\n",
    "        answer = chat_turn(user_input)\n",
    "        i += 1\n",
    "        print(f\"Assistant: {answer}\\n\")\n",
    "        print({answer})\n",
    "\n",
    "        # Example questions are: where were the last olympics held?  who won the world cup in 2022?  Who wa the team captain? what is the capital of france? who is the president of the united states? what is the tallest mountain in the world?\n",
    "        # who won the women's 100m  hurldes at the 2025 tokyo world athletics championship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42a4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather response \n",
      "Weather response tool calls [{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_vEseQaMcTOjXAwdAzOXLxP1L', 'type': 'tool_call'}]\n",
      "Puzzle response A pound of feathers and a pound of gold both weigh the same—they each weigh one pound. \n",
      "\n",
      "However, there is an interesting detail: in the past, gold and other precious metals were sometimes weighed using a different system called the troy pound, which is slightly lighter than the common avoirdupois pound used for most other items. \n",
      "\n",
      "- 1 avoirdupois pound = 16 ounces = 453.6 grams (used for feathers)\n",
      "- 1 troy pound = 12 ounces = 373.2 grams (used for gold)\n",
      "\n",
      "So if you use these traditional systems, a \"pound\" of feathers (avoirdupois) would actually weigh more than a \"pound\" of gold (troy). But if you’re simply talking about one pound (without specifying systems), they weigh the same.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class WeatherOutput(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "def get_weather(location: str) -> WeatherOutput: # Here note that unlike inbuilt tools like web_search which OpenAI run server side, these function tools run client side and so currently langchain doesn't automatically run them for us\n",
    "    \"\"\"Gets the weather at a location\"\"\"\n",
    "    return WeatherOutput(\n",
    "        answer= f\"It's sunny in {location}.\",\n",
    "        justification=\"Mocking weather API that always returns sunny for demo.\"\n",
    "    )\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    \"\"\"Schema for response.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "structured_llm = llm.bind_tools(\n",
    "    [get_weather],)\n",
    "    \n",
    "#     response_format=OutputSchema,\n",
    "#     strict=True,\n",
    "# )\n",
    "\n",
    "# Response contains tool calls:\n",
    "tool_call_response = structured_llm.invoke(\"What is the weather in San Francisco?\")\n",
    "print(\"Weather response\", tool_call_response.content)\n",
    "print(\"Weather response tool calls\", tool_call_response.tool_calls)\n",
    "# structured_response.additional_kwargs[\"parsed\"] contains parsed output\n",
    "structured_response = structured_llm.invoke(\n",
    "    \"What weighs more, a pound of feathers or a pound of gold?\"\n",
    ")\n",
    "print(\"Puzzle response\", structured_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c65ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'tool_calls': [{'id': 'call_uSiXJrCzpWzCqU9Od2cpbIHc', 'function': {'arguments': '{\"location\":\"San Francisco\"}', 'name': 'get_weather', 'parsed_arguments': {'location': 'San Francisco'}}, 'type': 'function'}], 'parsed': None, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 103, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_62bb2e3c55', 'id': 'chatcmpl-CIvT7FFyWwryj5XDuHZd8R6lpLPOW', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--c01b8409-6f2b-4fbb-9265-0caa0df1ce95-0' tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_uSiXJrCzpWzCqU9Od2cpbIHc', 'type': 'tool_call'}] usage_metadata={'input_tokens': 103, 'output_tokens': 15, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Weather response \n",
      "Weather response tool calls [{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_uSiXJrCzpWzCqU9Od2cpbIHc', 'type': 'tool_call'}]\n",
      "\n",
      " content='{\"answer\":\"A pound of feathers and a pound of gold weigh the same: one pound.\",\"justification\":\"The question compares weight units. Regardless of the material, a pound is a pound. However, in some contexts, precious metals like gold are weighed using the troy pound (12 troy ounces, approx. 373 grams), while common goods like feathers use the avoirdupois pound (16 ounces, approx. 454 grams). So, by strict measurement standards, a pound of feathers (avoirdupois) actually weighs more than a pound of gold (troy), but if both are measured by the same pound unit, they weigh the same.\"}' additional_kwargs={'parsed': OutputSchema(answer='A pound of feathers and a pound of gold weigh the same: one pound.', justification='The question compares weight units. Regardless of the material, a pound is a pound. However, in some contexts, precious metals like gold are weighed using the troy pound (12 troy ounces, approx. 373 grams), while common goods like feathers use the avoirdupois pound (16 ounces, approx. 454 grams). So, by strict measurement standards, a pound of feathers (avoirdupois) actually weighs more than a pound of gold (troy), but if both are measured by the same pound unit, they weigh the same.'), 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 139, 'prompt_tokens': 109, 'total_tokens': 248, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_62bb2e3c55', 'id': 'chatcmpl-CIvT8zjSA6oWG7MzxABbjEpTHQzb4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--4862a733-f401-4535-9d83-d862fcf3c4bf-0' usage_metadata={'input_tokens': 109, 'output_tokens': 139, 'total_tokens': 248, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Puzzle response {\"answer\":\"A pound of feathers and a pound of gold weigh the same: one pound.\",\"justification\":\"The question compares weight units. Regardless of the material, a pound is a pound. However, in some contexts, precious metals like gold are weighed using the troy pound (12 troy ounces, approx. 373 grams), while common goods like feathers use the avoirdupois pound (16 ounces, approx. 454 grams). So, by strict measurement standards, a pound of feathers (avoirdupois) actually weighs more than a pound of gold (troy), but if both are measured by the same pound unit, they weigh the same.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class WeatherOutput(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "@tool #I tried using the tool decorator but it made no difference\n",
    "def get_weather(location: str) -> WeatherOutput: # Here note that unlike inbuilt tools like web_search which OpenAI run server side, these function tools run client side and so currently langchain doesn't automatically run them for us\n",
    "    \"\"\"Gets the weather at a location\"\"\"\n",
    "    return WeatherOutput(\n",
    "        answer= f\"It's sunny in {location}.\",\n",
    "        justification=\"Mocking weather API that always returns sunny for demo.\"\n",
    "     )\n",
    "\n",
    "class OutputSchema(BaseModel):\n",
    "    \"\"\"Schema for response.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "structured_llm = llm.bind_tools(\n",
    "    [get_weather],\n",
    "    \n",
    "    response_format=OutputSchema,\n",
    "    strict=True,\n",
    ")\n",
    "\n",
    "# Response contains tool calls:\n",
    "tool_call_response = structured_llm.invoke(\"What is the weather in San Francisco?\")\n",
    "print(tool_call_response)\n",
    "print(\"Weather response\", tool_call_response.content)\n",
    "print(\"Weather response tool calls\", tool_call_response.tool_calls)\n",
    "# structured_response.additional_kwargs[\"parsed\"] contains parsed output\n",
    "structured_response = structured_llm.invoke(\n",
    "    \"What weighs more, a pound of feathers or a pound of gold?\"\n",
    ")\n",
    "print(\"\\n\",structured_response)\n",
    "print(\"Puzzle response\", structured_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45f8ba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.33\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(version(\"langchain-openai\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fd42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='call_0jN7uAcaQH5wD2ZYvEFxtV0H', function=Function(arguments='{\"city\":\"London\"}', name='get_weather'), type='function')])\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI #just trying out the OpenAI client. OpenAI client is not as friendly as langchain but it works\n",
    "client = OpenAI()\n",
    "\n",
    "# Example of a function tool call\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the weather for a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"city\": {\"type\": \"string\"}},\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather in London?\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "\n",
    "print(resp.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46652fe",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "888010fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the embeddings client\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=3072)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ebd0ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: get an embedding as a NumPy array\n",
    "def embed(text: str) -> np.ndarray:\n",
    "    vec = embeddings.embed_query(text)   # returns a Python list of floats\n",
    "    return np.array(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcd812fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the words of interest\n",
    "king = embed(\"king\")\n",
    "queen = embed(\"queen\")\n",
    "man = embed(\"man\")\n",
    "woman = embed(\"woman\")\n",
    "\n",
    "Paris = embed(\"Paris\")\n",
    "France = embed(\"France\")\n",
    "Italy = embed(\"Italy\")\n",
    "Rome= embed(\"Rome\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e184ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between true queen and computed vector: 0.5214\n"
     ]
    }
   ],
   "source": [
    "# Word math: queen ≈ woman + king − man\n",
    "approx_queen = woman + king - man\n",
    "\n",
    "# Compare cosine similarity\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarity = cosine_similarity(queen, approx_queen)\n",
    "print(f\"Cosine similarity between true queen and computed vector: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1520030",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "54dad2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', np.float64(0.6903742809749485)),\n",
       " ('woman', np.float64(0.5696693490830003)),\n",
       " ('queen', np.float64(0.5213563243282194)),\n",
       " ('man', np.float64(-0.008197622219882548))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for illustration: rank nearest among our 4 words\n",
    "vectors = {\"king\": king, \"queen\": queen, \"man\": man, \"woman\": woman}\n",
    "scores = {word: cosine_similarity(approx_queen, vec) for word, vec in vectors.items()}\n",
    "sorted(scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01bbb40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6983634582040057\n"
     ]
    }
   ],
   "source": [
    "approx_Rome= France + Rome - Italy\n",
    "print(cosine_similarity(Rome, approx_Rome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3b054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A man is playing a guitar.', np.float64(0.9999999999999999)),\n",
       " ('Someone is strumming an instrument.', np.float64(0.5117938139436554)),\n",
       " ('A woman is cooking dinner.', np.float64(0.2242725686883578)),\n",
       " ('The weather is sunny and warm.', np.float64(0.14281628743630567))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 1: Semantic similarity of paraphrases\n",
    "sents = [\n",
    "    \"A man is playing a guitar.\",\n",
    "    \"Someone is strumming an instrument.\",\n",
    "    \"The weather is sunny and warm.\",\n",
    "    \"A woman is cooking dinner.\"\n",
    "]\n",
    "\n",
    "vecs = {s: embed(s) for s in sents}\n",
    "target = vecs[\"A man is playing a guitar.\"]\n",
    "\n",
    "sims = [(s, cosine_similarity(target, v)) for s,v in vecs.items()]\n",
    "sorted(sims, key=lambda x: -x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a5da663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The football match ended in a draw.', np.float64(0.40843571805118306)),\n",
       " ('Arsenal won 3-2.', np.float64(0.38845099768817515)),\n",
       " ('Arsenal beat Liverpool 3-2.', np.float64(0.3652957204670271)),\n",
       " ('The football match went into extra time.', np.float64(0.3592746323447075)),\n",
       " ('A basketball player scored a triple-double.',\n",
       "  np.float64(0.2747090663570589)),\n",
       " ('The experiment confirmed the hypothesis.', np.float64(0.12043236856133151)),\n",
       " ('Researchers discovered a new species of frog.',\n",
       "  np.float64(0.07104991386677663))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 2: Topic grouping\n",
    "sports = [\n",
    "    \"The football match went into extra time.\",\n",
    "    \"The football match ended in a draw.\",\n",
    "    \"Arsenal won 3-2.\",\n",
    "    \"Arsenal beat Liverpool 3-2.\",\n",
    "    \"A basketball player scored a triple-double.\"\n",
    "]\n",
    "science = [\n",
    "    \"The experiment confirmed the hypothesis.\",\n",
    "    \"Researchers discovered a new species of frog.\"\n",
    "]\n",
    "all_sents = sports + science\n",
    "\n",
    "vecs = {s: embed(s) for s in all_sents}\n",
    "target = embed(\"Who won the soccer game yesterday?\")\n",
    "\n",
    "sims = [(s, cosine_similarity(target, v)) for s,v in vecs.items()]\n",
    "sorted(sims, key=lambda x: -x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb6e45e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mount Everest is the tallest mountain in the world.',\n",
       "  np.float64(0.5875616182838077)),\n",
       " ('The Grand Canyon is the lowest trough in the world. ',\n",
       "  np.float64(0.24209245155821002)),\n",
       " ('Chinese is the most spoken language in the world.',\n",
       "  np.float64(0.1528612564098758)),\n",
       " ('The Mona Lisa is a famous painting by Leonardo da Vinci.',\n",
       "  np.float64(0.09994392241515195)),\n",
       " ('Python is a popular programming language for machine learning.',\n",
       "  np.float64(0.05047505054960984))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4: Document search toy demo\n",
    "docs = [\n",
    "    \"The Mona Lisa is a famous painting by Leonardo da Vinci.\",\n",
    "    \"Mount Everest is the tallest mountain in the world.\",\n",
    "    \"The Grand Canyon is the lowest trough in the world. \",\n",
    "    \"Chinese is the most spoken language in the world.\",\n",
    "    \"Python is a popular programming language for machine learning.\"\n",
    "]\n",
    "\n",
    "vecs = {d: embed(d) for d in docs}\n",
    "query = \"Which mountain is the highest on Earth?\"\n",
    "qvec = embed(query)\n",
    "\n",
    "sims = [(d, cosine_similarity(qvec, v)) for d,v in vecs.items()]\n",
    "sorted(sims, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1187e6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ottawa is the capital of Canada.', np.float64(0.7563769556955102)),\n",
       " ('Paris is the capital of France.', np.float64(0.6828128201082201)),\n",
       " ('Tokyo is the capital of Japan.', np.float64(0.5335522161467855)),\n",
       " ('Berlin is the capital of Germany.', np.float64(0.43951000420692404))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"Paris is the capital of France.\"\n",
    "s2 = \"Berlin is the capital of Germany.\"\n",
    "s3 = \"Ottawa is the capital of Canada.\"\n",
    "s4 = \"Europe.\"\n",
    "\n",
    "vec_math = embed(s1) - embed(s4) + embed(s3)\n",
    "\n",
    "# Compare to candidate sentences\n",
    "candidates = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Berlin is the capital of Germany.\",\n",
    "    \"Ottawa is the capital of Canada.\",\n",
    "    \"Tokyo is the capital of Japan.\"\n",
    "]\n",
    "\n",
    "vecs = {c: embed(c) for c in candidates}\n",
    "scores = {c: cosine_similarity(vec_math, v) for c,v in vecs.items()}\n",
    "sorted(scores.items(), key=lambda x: -x[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intern_GenAI_Examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
